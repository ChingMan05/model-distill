# ğŸ§  Qwen2 çŸ¥è¯†è’¸é¦è„šæœ¬ï¼ˆä½¿ç”¨ TextBrewer æ¡†æ¶ï¼‰

æœ¬é¡¹ç›®ä½¿ç”¨ [TextBrewer](https://github.com/airaria/TextBrewer) æ¡†æ¶ï¼Œå¯¹ **Qwen2-7B** æ•™å¸ˆæ¨¡å‹è¿›è¡ŒçŸ¥è¯†è’¸é¦ï¼Œå¾—åˆ° **Qwen2-1.5B** å­¦ç”Ÿæ¨¡å‹ã€‚  
è’¸é¦ä»»åŠ¡ä¸º **è¯­è¨€å»ºæ¨¡ï¼ˆLanguage Modelingï¼‰**ï¼Œåœ¨ **WikiText-2** æ•°æ®é›†ä¸Šè¿›è¡Œã€‚

---

## ğŸš€ é¡¹ç›®ç®€ä»‹

### ğŸ¯ ç›®æ ‡

- å°† Qwen2-7B çš„è¯­è¨€å»ºæ¨¡çŸ¥è¯†è¿ç§»åˆ° Qwen2-1.5B
    
- ä½¿ç”¨ **soft label è’¸é¦ï¼ˆKD lossï¼‰**ï¼Œæå‡å­¦ç”Ÿæ¨¡å‹çš„è¯­è¨€ç†è§£ä¸ç”Ÿæˆèƒ½åŠ›
    
- å¯¹é½æ•™å¸ˆä¸å­¦ç”Ÿæ¨¡å‹çš„ logitsï¼Œé¿å…ç»´åº¦ä¸åŒ¹é…é—®é¢˜
    

### âš™ï¸ æ ¸å¿ƒç‰¹æ€§

- âœ… ä½¿ç”¨ **TextBrewer** æ¡†æ¶è¿›è¡ŒçŸ¥è¯†è’¸é¦
    
- âœ… è‡ªåŠ¨å¯¹é½æ•™å¸ˆ/å­¦ç”Ÿæ¨¡å‹çš„è¾“å‡º logits
    
- âœ… æ”¯æŒå­é›†/å®Œæ•´æ•°æ®è®­ç»ƒæ¨¡å¼åˆ‡æ¢
    
- âœ… è‡ªå®šä¹‰ KD Loss å®ç°
    
- âœ… è‡ªåŠ¨ä¿å­˜è’¸é¦æ¨¡å‹ä¸ tokenizer
    

---

## ğŸ§© ç¯å¢ƒä¾èµ–

```bash
conda create -n distill python=3.10
conda activate distill

pip install torch transformers datasets textbrewer accelerate

```
å»ºè®®ä½¿ç”¨ GPU ç¯å¢ƒï¼Œå¹¶å¼€å¯å¤šå¡è®­ç»ƒï¼ˆç¤ºä¾‹è„šæœ¬é»˜è®¤ä½¿ç”¨ GPU 0 å’Œ 1ï¼‰ã€‚

---

## ğŸ“ ç›®å½•ç»“æ„

```bash
/root/autodl-tmp/
â”‚
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ Qwen2-7B/           # æ•™å¸ˆæ¨¡å‹
â”‚   â””â”€â”€ Qwen2-1.5B/         # å­¦ç”Ÿæ¨¡å‹
â”‚
â”œâ”€â”€ datasets/
â”‚   â””â”€â”€ wikitext-2-raw-v1/  # æ•°æ®é›†ç¼“å­˜ç›®å½•
â”‚
â””â”€â”€ model-distill/
    â”œâ”€â”€ distill_qwen2.py     # è’¸é¦ä¸»è„šæœ¬
    â””â”€â”€ README.md            # æœ¬è¯´æ˜æ–‡ä»¶

```
---

## ğŸ“¦ è¿è¡Œè¯´æ˜

### 1ï¸âƒ£ è®¾ç½®è·¯å¾„

åœ¨è„šæœ¬ä¸­ä¿®æ”¹ä»¥ä¸‹å˜é‡ä»¥åŒ¹é…ä½ çš„ç¯å¢ƒï¼š

```python
teacher_path = "/root/autodl-tmp/models/Qwen2-7B"
student_path = "/root/autodl-tmp/models/Qwen2-1.5B"
dataset_path = "/root/autodl-tmp/datasets/wikitext-2-raw-v1"

```

---

### 2ï¸âƒ£ é€‰æ‹©æ•°æ®è§„æ¨¡

å¿«é€Ÿæµ‹è¯•å¯ä»…ä½¿ç”¨éƒ¨åˆ†æ ·æœ¬ï¼ˆé»˜è®¤ 5000 æ¡ï¼‰ï¼š

```python
USE_FULL_DATA = False  # æ”¹ä¸º True ä½¿ç”¨å…¨éƒ¨æ•°æ®
```

---

### 3ï¸âƒ£ å¯åŠ¨è’¸é¦è®­ç»ƒ

```python
python distill_qwen2.py
```

è®­ç»ƒæ—¥å¿—å°†è‡ªåŠ¨ä¿å­˜è‡³ï¼š

```python
./logs_qwen2/
```

---

## ğŸ” è®­ç»ƒé…ç½®æ‘˜è¦

|é…ç½®é¡¹|å€¼|
|---|---|
|æ‰¹å¤§å°|2|
|æ¢¯åº¦ç´¯ç§¯æ­¥æ•°|8|
|æ¸©åº¦ï¼ˆTemperatureï¼‰|2.0|
|KD Loss æƒé‡|1.0|
|ç¡¬æ ‡ç­¾æƒé‡|0.0|
|ä¼˜åŒ–å™¨|AdamW|
|å­¦ä¹ ç‡|2e-5|
|è®­ç»ƒè½®æ•°|1|

---

## ğŸ§  è’¸é¦æœºåˆ¶è¯´æ˜

### è‡ªå®šä¹‰ KD Loss

ä½¿ç”¨æ¸©åº¦ç¼©æ”¾çš„äº¤å‰ç†µæŸå¤±ï¼ˆsoft targetï¼‰ï¼š

```python
loss = -(p_T * log_softmax(logits_S / T)).sum().mean() * T^2
```

### å¯¹é½æœºåˆ¶

è’¸é¦è¿‡ç¨‹ä¸­ï¼Œæ•™å¸ˆä¸å­¦ç”Ÿçš„è¾“å‡ºç»´åº¦å¯èƒ½ä¸åŒï¼š

- **åºåˆ—é•¿åº¦ä¸ä¸€è‡´** â†’ æˆªæ–­åˆ°æœ€çŸ­é•¿åº¦
    
- **è¯è¡¨å¤§å°ä¸ä¸€è‡´** â†’ æˆªæ–­åˆ°æœ€å°è¯è¡¨ç»´åº¦
    
- è‡ªåŠ¨æ‰“å°ä¸€æ¬¡å¯¹é½ä¿¡æ¯ï¼Œä¾‹å¦‚ï¼š
    
```css
    [ALIGN] Teacher torch.Size([2, 256, 152064]) â†’ Student torch.Size([2, 256, 151936]) â†’ Truncated to torch.Size([2, 256, 151936])

```
    

---

## ğŸ’¾ æ¨¡å‹ä¿å­˜

è®­ç»ƒå®Œæˆåè‡ªåŠ¨ä¿å­˜ï¼š

`./distilled_qwen2_1.5B_fix/`

ç›®å½•ä¸­åŒ…å«ï¼š

- `pytorch_model.bin`ï¼šè’¸é¦åçš„å­¦ç”Ÿæ¨¡å‹æƒé‡
    
- `config.json`ï¼šæ¨¡å‹é…ç½®æ–‡ä»¶
    
- `tokenizer.json` / `vocab.json`ï¼šåˆ†è¯å™¨æ–‡ä»¶
    

---

## ğŸ“Š ç¤ºä¾‹è¾“å‡º

```yaml
ğŸš€ Starting dynamic distillation (with safe alignment)...
ğŸ“Š Training info:
   - Total batches: 2500
   - Batch size: 2
   - Gradient accumulation: 8
   - Effective batch size: 16
   - Total samples: 5000
   - Estimated steps: 312

[ALIGN] Teacher torch.Size([2, 256, 152064]) â†’ Student torch.Size([2, 256, 151936]) â†’ Truncated to torch.Size([2, 256, 151936])
âœ… è’¸é¦å®Œæˆï¼Œæ¨¡å‹ä¿å­˜è‡³: ./distilled_qwen2_1.5B_fix

```



