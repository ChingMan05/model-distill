import os
import torch
import numpy as np
from transformers import AutoModelForCausalLM, AutoTokenizer
from datasets import load_dataset
from tqdm import tqdm
import torch.nn.functional as F

os.environ["CUDA_VISIBLE_DEVICES"] = "0"

# ==================== è·¯å¾„é…ç½® ====================
teacher_path = "/root/autodl-tmp/models/Qwen2-7B"
student_original_path = "/root/autodl-tmp/models/Qwen2-1.5B"
student_distilled_path = "/root/autodl-tmp/model-distill/distilled_qwen2_1.5B_full_20251023_103726"
dataset_path = "/root/autodl-tmp/datasets/wikitext-2-raw-v1"

device = "cuda" if torch.cuda.is_available() else "cpu"

# ==================== 1. å›°æƒ‘åº¦è¯„ä¼° (Perplexity) ====================
def calculate_perplexity(model, tokenizer, texts, max_length=256):
    """è®¡ç®—æ¨¡å‹åœ¨æµ‹è¯•é›†ä¸Šçš„å›°æƒ‘åº¦"""
    model.eval()
    total_loss = 0
    total_tokens = 0
    
    with torch.no_grad():
        for text in tqdm(texts, desc="Computing perplexity"):
            if not text.strip():
                continue
                
            inputs = tokenizer(
                text, 
                return_tensors="pt", 
                truncation=True, 
                max_length=max_length
            ).to(device)
            
            outputs = model(**inputs, labels=inputs["input_ids"])
            loss = outputs.loss
            
            # ç´¯è®¡ loss å’Œ token æ•°
            total_loss += loss.item() * inputs["input_ids"].size(1)
            total_tokens += inputs["input_ids"].size(1)
    
    avg_loss = total_loss / total_tokens
    perplexity = np.exp(avg_loss)
    return perplexity

# ==================== 2. è¾“å‡ºç›¸ä¼¼åº¦è¯„ä¼° (KL Divergence) ====================
def calculate_kl_divergence(teacher, student, tokenizer, texts, max_length=256):
    """è®¡ç®—å­¦ç”Ÿå’Œæ•™å¸ˆè¾“å‡ºçš„ KL æ•£åº¦"""
    teacher.eval()
    student.eval()
    
    total_kl = 0
    count = 0
    
    with torch.no_grad():
        for text in tqdm(texts, desc="Computing KL divergence"):
            if not text.strip():
                continue
                
            inputs = tokenizer(
                text, 
                return_tensors="pt", 
                truncation=True, 
                max_length=max_length
            ).to(device)
            
            # è·å– teacher å’Œ student çš„ logits
            teacher_outputs = teacher(**inputs)
            student_outputs = student(**inputs)
            
            teacher_logits = teacher_outputs.logits
            student_logits = student_outputs.logits
            
            # å¯¹é½è¯è¡¨å¤§å°
            min_vocab = min(teacher_logits.size(-1), student_logits.size(-1))
            teacher_logits = teacher_logits[:, :, :min_vocab]
            student_logits = student_logits[:, :, :min_vocab]
            
            # è®¡ç®— KL æ•£åº¦
            teacher_probs = F.softmax(teacher_logits, dim=-1)
            student_log_probs = F.log_softmax(student_logits, dim=-1)
            
            kl = (teacher_probs * (teacher_probs.log() - student_log_probs)).sum(dim=-1).mean()
            total_kl += kl.item()
            count += 1
    
    return total_kl / count if count > 0 else float('inf')

# ==================== 3. ç”Ÿæˆè´¨é‡è¯„ä¼° ====================
def evaluate_generation(model, tokenizer, prompts):
    """è¯„ä¼°ç”Ÿæˆè´¨é‡"""
    model.eval()
    results = []
    
    for prompt in prompts:
        inputs = tokenizer(prompt, return_tensors="pt").to(device)
        
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_new_tokens=50,
                do_sample=True,
                temperature=0.7,
                top_p=0.9,
                pad_token_id=tokenizer.eos_token_id
            )
        
        generated = tokenizer.decode(outputs[0], skip_special_tokens=True)
        results.append({
            "prompt": prompt,
            "generated": generated
        })
    
    return results

# ==================== 4. ä¸»è¯„ä¼°æµç¨‹ ====================
def main():
    print("="*80)
    print("ğŸ” å¼€å§‹è¯„ä¼°è’¸é¦æ•ˆæœ")
    print("="*80)
    
    # åŠ è½½æ¨¡å‹
    print("\nğŸ“¦ Loading models...")
    tokenizer = AutoTokenizer.from_pretrained(teacher_path, trust_remote_code=True)
    
    teacher = AutoModelForCausalLM.from_pretrained(
        teacher_path,
        torch_dtype=torch.bfloat16,
        device_map="auto",
        trust_remote_code=True
    ).eval()
    
    student_original = AutoModelForCausalLM.from_pretrained(
        student_original_path,
        torch_dtype=torch.bfloat16,
        device_map="auto",
        trust_remote_code=True
    ).eval()
    
    student_distilled = AutoModelForCausalLM.from_pretrained(
        student_distilled_path,
        torch_dtype=torch.bfloat16,
        device_map="auto",
        trust_remote_code=True
    ).eval()
    
    # åŠ è½½æµ‹è¯•æ•°æ®
    print("\nğŸ“š Loading test dataset...")
    dataset = load_dataset("wikitext", "wikitext-2-raw-v1", cache_dir=dataset_path)
    test_texts = [t for t in dataset["test"]["text"][:500] if t.strip()]  # ä½¿ç”¨ 500 æ¡æµ‹è¯•
    
    # ==================== è¯„ä¼° 1: å›°æƒ‘åº¦ ====================
    print("\n" + "="*80)
    print("ğŸ“Š è¯„ä¼° 1: å›°æƒ‘åº¦ (Perplexity) - è¶Šä½è¶Šå¥½")
    print("="*80)
    
    print("\nğŸ”¹ Teacher (Qwen2-7B)...")
    teacher_ppl = calculate_perplexity(teacher, tokenizer, test_texts)
    
    print("\nğŸ”¹ Student Original (Qwen2-1.5B åŸå§‹)...")
    student_orig_ppl = calculate_perplexity(student_original, tokenizer, test_texts)
    
    print("\nğŸ”¹ Student Distilled (Qwen2-1.5B è’¸é¦å)...")
    student_dist_ppl = calculate_perplexity(student_distilled, tokenizer, test_texts)
    
    print("\n" + "-"*80)
    print(f"Teacher Perplexity:           {teacher_ppl:.2f}")
    print(f"Student Original Perplexity:  {student_orig_ppl:.2f}")
    print(f"Student Distilled Perplexity: {student_dist_ppl:.2f}")
    print(f"Improvement:                  {student_orig_ppl - student_dist_ppl:.2f} ({'â†“' if student_dist_ppl < student_orig_ppl else 'â†‘'})")
    print(f"Gap to Teacher:               {student_dist_ppl - teacher_ppl:.2f}")
    print("-"*80)
    
    # ==================== è¯„ä¼° 2: KL æ•£åº¦ ====================
    print("\n" + "="*80)
    print("ğŸ“Š è¯„ä¼° 2: ä¸æ•™å¸ˆçš„ KL æ•£åº¦ - è¶Šä½è¶Šå¥½ï¼ˆè¡¨ç¤ºè¶Šæ¥è¿‘æ•™å¸ˆï¼‰")
    print("="*80)
    
    print("\nğŸ”¹ Student Original vs Teacher...")
    kl_orig = calculate_kl_divergence(teacher, student_original, tokenizer, test_texts[:100])
    
    print("\nğŸ”¹ Student Distilled vs Teacher...")
    kl_dist = calculate_kl_divergence(teacher, student_distilled, tokenizer, test_texts[:100])
    
    print("\n" + "-"*80)
    print(f"Student Original KL:  {kl_orig:.4f}")
    print(f"Student Distilled KL: {kl_dist:.4f}")
    print(f"Improvement:          {kl_orig - kl_dist:.4f} ({'â†“' if kl_dist < kl_orig else 'â†‘'})")
    print("-"*80)
    
    # ==================== è¯„ä¼° 3: ç”Ÿæˆè´¨é‡ ====================
    print("\n" + "="*80)
    print("ğŸ“Š è¯„ä¼° 3: ç”Ÿæˆè´¨é‡å¯¹æ¯”")
    print("="*80)
    
    test_prompts = [
        "The history of artificial intelligence began",
        "In recent years, climate change has",
        "The key to successful machine learning is"
    ]
    
    print("\nğŸ”¹ Generating with Teacher...")
    teacher_results = evaluate_generation(teacher, tokenizer, test_prompts)
    
    print("\nğŸ”¹ Generating with Student Original...")
    student_orig_results = evaluate_generation(student_original, tokenizer, test_prompts)
    
    print("\nğŸ”¹ Generating with Student Distilled...")
    student_dist_results = evaluate_generation(student_distilled, tokenizer, test_prompts)
    
    # æ‰“å°å¯¹æ¯”ç»“æœ
    for i, prompt in enumerate(test_prompts):
        print(f"\n{'='*80}")
        print(f"Prompt {i+1}: {prompt}")
        print(f"{'='*80}")
        print(f"\nğŸ“ Teacher:\n{teacher_results[i]['generated']}")
        print(f"\nğŸ“˜ Student Original:\n{student_orig_results[i]['generated']}")
        print(f"\nğŸ“— Student Distilled:\n{student_dist_results[i]['generated']}")
    
    # ==================== æ€»ç»“ ====================
    print("\n" + "="*80)
    print("ğŸ“ è¯„ä¼°æ€»ç»“")
    print("="*80)
    
    improvements = []
    if student_dist_ppl < student_orig_ppl:
        improvements.append(f"âœ… å›°æƒ‘åº¦é™ä½ {student_orig_ppl - student_dist_ppl:.2f}")
    else:
        improvements.append(f"âŒ å›°æƒ‘åº¦ä¸Šå‡ {student_dist_ppl - student_orig_ppl:.2f}")
    
    if kl_dist < kl_orig:
        improvements.append(f"âœ… KL æ•£åº¦é™ä½ {kl_orig - kl_dist:.4f} (æ›´æ¥è¿‘æ•™å¸ˆ)")
    else:
        improvements.append(f"âŒ KL æ•£åº¦ä¸Šå‡ {kl_dist - kl_orig:.4f}")
    
    print("\næ”¹è¿›æƒ…å†µ:")
    for imp in improvements:
        print(f"  {imp}")
    
    print("\næ¨¡å‹å¤§å°å¯¹æ¯”:")
    print(f"  Teacher:  7B å‚æ•°")
    print(f"  Student:  1.5B å‚æ•° (21% å¤§å°)")
    
    print("\n" + "="*80)
    print("âœ… è¯„ä¼°å®Œæˆï¼")
    print("="*80)

if __name__ == "__main__":
    main()